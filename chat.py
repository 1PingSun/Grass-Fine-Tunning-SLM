from transformers import AutoTokenizer, AutoModelForCausalLM
from TTS.tts.configs.xtts_config import XttsConfig
from TTS.tts.models.xtts import Xtts
import torch
import pygame
import pyaudio       
from start_recording import sr

def ttsc(text):
    config = XttsConfig()
    config.load_json("XTTS-v2/config.json")
    model = Xtts.init_from_config(config)
    model.load_checkpoint(config, checkpoint_dir="XTTS-v2/", eval=True)
    # ç¡®ä¿ä½¿ç”¨GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if torch.cuda.is_available():
        model.cuda()

    try:
        outputs = model.synthesize(
            text,
            config,
            speaker_wav="test.wav",
            gpt_cond_len=3,
            language="zh-cn",
        )
        # å°†éŸ³é¢‘ä¿å­˜åˆ°æ–‡ä»¶
        import scipy
        scipy.io.wavfile.write("output_speech.wav", rate=24000, data=outputs["wav"])
        return "output_speech.wav"
        #print("è¯­éŸ³åˆæˆæˆåŠŸï¼Œå·²ä¿å­˜åˆ°output_speech.wav")
    except AttributeError as e:
        if "'GPT2InferenceModel' object has no attribute 'generate'" in str(e):
            print("é”™è¯¯ï¼štransformersåº“ç‰ˆæœ¬è¿‡é«˜ï¼Œè¯·é™çº§åˆ°4.49.0ç‰ˆæœ¬")
            print("è¯·è¿è¡Œ: pip install transformers==4.49.0")
        else:
            raise e




model_path = "Qwen2.5-0.5B-Counseling"  

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float32,  # è‹¥ GPU æ”¯æ´ FP16ï¼Œå¯ä½¿ç”¨ã€‚å¦å‰‡æ”¹ç‚º torch.float32
    device_map="auto",
    trust_remote_code=True
)
chat_history = []

print("ğŸ’¬ æ¨¡å‹å·²å•Ÿå‹•ï¼Œè¼¸å…¥ 'exit' çµæŸèŠå¤©ã€‚")
while True:
    # user_input = input("ğŸ§‘ ä½ ï¼š")
    input("è«‹æŒ‰ä¸‹ Enter é–‹å§‹éŒ„éŸ³ä¸¦è¾¨è­˜...")
    user_input = sr()  # æŒ‰ä¸‹ Enter å¾Œæ‰åŸ·è¡Œé€™è¡Œ
    print("è¾¨è­˜çµæœï¼š", user_input)
    if user_input.lower() == "exit":
        print("ğŸ‘‹ å†è¦‹ï¼")
        break

    chat_history.append(f"User: {user_input}")
    prompt = "\n".join(chat_history) + "\nAssistant:"

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=20,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )

    decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    response = decoded[len(prompt):].strip().split("User:")[0].strip()
    wavload = ttsc(response)
    print(response)
    pygame.mixer.init()
    pygame.mixer.music.load(wavload)
    pygame.mixer.music.play()
    while pygame.mixer.music.get_busy():
        pygame.time.Clock().tick(10)
    #print("ğŸ¤– æ¨¡å‹ï¼š", response)
    chat_history.append(f"Assistant: {response}")
