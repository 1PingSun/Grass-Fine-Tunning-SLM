from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_path = "Qwen2.5-0.5B-Counseling"  

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float32,  # è‹¥ GPU æ”¯æ´ FP16ï¼Œå¯ä½¿ç”¨ã€‚å¦å‰‡æ”¹ç‚º torch.float32
    device_map="auto",
    trust_remote_code=True
)

chat_history = []

print("ğŸ’¬ æ¨¡å‹å·²å•Ÿå‹•ï¼Œè¼¸å…¥ 'exit' çµæŸèŠå¤©ã€‚")
while True:
    user_input = input("ğŸ§‘ ä½ ï¼š")
    if user_input.lower() == "exit":
        print("ğŸ‘‹ å†è¦‹ï¼")
        break

    chat_history.append(f"User: {user_input}")
    prompt = "\n".join(chat_history) + "\nAssistant:"

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )

    decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    response = decoded[len(prompt):].strip().split("User:")[0].strip()
    print("ğŸ¤– æ¨¡å‹ï¼š", response)
    chat_history.append(f"Assistant: {response}")
