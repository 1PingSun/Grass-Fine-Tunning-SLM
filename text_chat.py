from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# è¼‰å…¥æ¨¡å‹
model_path = "Qwen2.5-0.5B-Counseling"  

print("ğŸ”„ æ­£åœ¨è¼‰å…¥æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float32,  # è‹¥ GPU æ”¯æ´ FP16ï¼Œå¯ä½¿ç”¨ã€‚å¦å‰‡æ”¹ç‚º torch.float32
    device_map="auto",
    trust_remote_code=True
)

# åˆå§‹åŒ–èŠå¤©æ­·å²
chat_history = []

print("ğŸ’¬ æ¨¡å‹å·²å•Ÿå‹•ï¼Œè¼¸å…¥ 'exit' çµæŸèŠå¤©ã€‚")
print("=" * 50)

while True:
    # ç²å–ç”¨æˆ¶è¼¸å…¥
    user_input = input("ğŸ§‘ ä½ ï¼š")
    
    # æª¢æŸ¥æ˜¯å¦è¦é€€å‡º
    if user_input.lower() == "exit":
        print("ğŸ‘‹ å†è¦‹ï¼")
        break
    
    # å¦‚æœè¼¸å…¥ç‚ºç©ºï¼Œè·³é
    if not user_input.strip():
        continue
    
    # å°‡ç”¨æˆ¶è¼¸å…¥æ·»åŠ åˆ°èŠå¤©æ­·å²
    chat_history.append(f"User: {user_input}")
    
    # æ§‹å»ºæç¤ºè©
    prompt = "\n".join(chat_history) + "\nAssistant:"
    
    # å°‡è¼¸å…¥è½‰æ›ç‚ºæ¨¡å‹å¯è™•ç†çš„æ ¼å¼
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # ç”Ÿæˆå›æ‡‰
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=50,  # å¢åŠ ç”Ÿæˆé•·åº¦ä»¥ç²å¾—æ›´å®Œæ•´çš„å›æ‡‰
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # è§£ç¢¼ä¸¦æå–å›æ‡‰
    decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    response = decoded[len(prompt):].strip().split("User:")[0].strip()
    
    # é¡¯ç¤ºæ¨¡å‹å›æ‡‰
    print(f"ğŸ¤– æ¨¡å‹ï¼š{response}")
    print("-" * 30)
    
    # å°‡æ¨¡å‹å›æ‡‰æ·»åŠ åˆ°èŠå¤©æ­·å²
    chat_history.append(f"Assistant: {response}")
    
    # é™åˆ¶èŠå¤©æ­·å²é•·åº¦ï¼Œé¿å…éé•·å½±éŸ¿æ€§èƒ½
    if len(chat_history) > 20:  # ä¿ç•™æœ€è¿‘10è¼ªå°è©±
        chat_history = chat_history[-20:]
